% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fastNaiveBayes.R, R/fastNaiveBayes.default.R
\name{fastNaiveBayes}
\alias{fastNaiveBayes}
\alias{fastNaiveBayes.default}
\title{Fast Naive Bayes Classifier}
\usage{
fastNaiveBayes(x, y, laplace = 0, sparse = FALSE, ...)

\method{fastNaiveBayes}{default}(x, y, laplace = 0, sparse = FALSE,
  distribution = c("bernoulli", "multinomial"), ...)
}
\arguments{
\item{x}{a numeric matrix with 1's and 0's to indicate the presence or absence of features. A sparse dgcMatrix is also accepted}

\item{y}{a factor of classes}

\item{laplace}{A number used for Laplace smoothing. Default is 0}

\item{sparse}{Use a sparse matrix? If true a sparse matrix will be constructed from x, which can give up to a 40% speed up.
It's possible to directly feed a sparse dgcMatrix as x, which will set this parameter to TRUE}

\item{...}{Not used.}
}
\value{
A fitted object of class "fastNaiveBayes". It has two components:

    \describe{
        \item{probability_table}{A list where each element is a list of probability vectors belonging to each class. That is each element
        of this list belongs to a certain class and each element is a list of it's own containing probability vectors. A probability vector
        is a vector where each element is the probability of a certain feature being absent/present
        }
        \item{names}{names of features used to train this fastNaiveBayes}
    }
}
\description{
Extremely fast and highly scalable implementation of a Naive Bayes Classifier. Computes the frequency
    tables of present and non present features corresponding to each class.
}
\details{
A standard Naive Bayes classifier that assumes independence between feature variables. Currently only numerical predictors
    That indicate the presence/absence of the feature is allowed. NA's are simply treated as 0, i.e. absent in all calculations.

    By setting sparse = TRUE the numeric matrix x will be converted to a sparse dgcMatrix. This is considerably fast in case only
    very few features are present,i.e. 1, and a lot of features are absent, i.e. 0. That is if there are a lot of 0's present in x
    using a sparse matrix will be faster.

    It's also possible to directly supply a sparse dgcMatrix, which is a lot faster in case
    a fastNaiveBayes model is trained multiple times on the same matrix or a subset of this. See examples for more details.
}
\examples{
    rm(list=ls())
    require(mlbench)
    require(Matrix)

    # Load BreastCancer data
    data(BreastCancer)
    dim(BreastCancer)
    levels(BreastCancer$Class)
    head(BreastCancer)

    # Convert to 0/1 dummies
    data_mat <- BreastCancer[,c("Class","Cl.thickness","Cell.size","Cell.shape")]
    col_counter <- ncol(data_mat)+1
    for(i in 2:ncol(data_mat)){
      for(val in unique(data_mat[,i])){
        data_mat[,col_counter] <- ifelse(data_mat[,i]==val,1,0)
        col_counter <- col_counter+1
      }
    }

    # Expand matrix to 10.000 rows
    data_mat <- data_mat[sample(1:nrow(data_mat), size = 10000, replace = TRUE),]

    # Construct y and sparse matrix
    y <- data_mat[,"Class"]
    data_mat <- data_mat[,setdiff(colnames(data_mat),c("Class","Cl.thickness",
    "Cell.size","Cell.shape"))]

    # Expand matrix to 1000 columns
    data_mat <- data_mat[,sample(1:ncol(data_mat),size=1000,replace = TRUE)]
    colnames(data_mat) <- paste0("var_",1:ncol(data_mat))

    sparse_data <- Matrix(as.matrix(data_mat), sparse = TRUE)
    data_mat <- as.matrix(data_mat)

    # Example to estimate and predict once:
    model <- fastNaiveBayes(data_mat[1:400,], y[1:400], laplace = 1, sparse = TRUE)
    preds <- predict(model, newdata = data_mat[401:nrow(data_mat),], type = "class")

    mean(preds!=y[401:length(y)])

    # Example, running model 100 times with sparse matrix directly. This will be faster when
    # large and sparse matrices are used
    start <- Sys.time()
    for(i in 1:100){
      train_idx <- sample(1:nrow(sparse_data), size = nrow(sparse_data), replace = TRUE)

      model <- fastNaiveBayes(sparse_data[train_idx,], y[train_idx], laplace=1)
      preds <- predict(model, sparse_data[-train_idx,], type = "class")
    }
    print(Sys.time()-start)

    # Example, data is a matrix and sparse set to true, which means at each function call, data
    # will converted to a sparse matrix
    start <- Sys.time()
    for(i in 1:100){
      train_idx <- sample(1:nrow(data_mat), size = nrow(data_mat), replace = TRUE)

      model <- fastNaiveBayes(data_mat[train_idx,], y[train_idx], laplace=1, sparse = TRUE)
      preds <- predict(model, data_mat[-train_idx,], type = "class", sparse = TRUE)
    }
    print(Sys.time()-start)

    # Last but not least, not using sparse matrix at all.
    start <- Sys.time()
    for(i in 1:100){
      train_idx <- sample(1:nrow(data_mat), size = nrow(data_mat), replace = TRUE)

      model <- fastNaiveBayes(data_mat[train_idx,], y[train_idx], laplace=1)
      preds <- predict(model, data_mat[-train_idx,], type = "class")
    }
    print(Sys.time()-start)
}
\seealso{
\code{\link{predict.fastNaiveBayes}} for the predict function for a fastNaiveBayes class.
}
